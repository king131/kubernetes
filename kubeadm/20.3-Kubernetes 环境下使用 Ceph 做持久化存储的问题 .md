由于 Pod 的生命周期结束后容器中存放的数据无法保证持久化，因此 K8S 一般需要配合分布式存储作为持久化方案来使用，经过简单调研我们公司选用的是 RedHat 的 Ceph。

但在自行配置整合 Kubernetes 与 Ceph 的过程中踩到了一些坑，然后这些坑在从头构建 K8S + Ceph 集群的过程中会反复遇到，所以这里做下记录。

另外如果业务上不要求自行配置 K8S + Ceph 集群的话，其实可以考虑使用一个基于 Kubernetes 和 Ceph 的存储方案 —— [rook](https://rook.io/)，开箱即用，非常方便。

### 创建 RBD Persistent Volume 失败
如果按照 `Kubernetes 1.10.0` 的文档直接配置 RBD 的 `storageClass` 再创建 PVC 的话会遇到如下错误：

> Failed to provision volume with StorageClass "": failed to create rbd image: executable file not found in $PATH, command output:

根据这个 issue （[kubernetes/kubernetes#38923](https://github.com/kubernetes/kubernetes/issues/38923) ）报告的情况，出错的原因是 `kube-controller-manager` 容器中不包含 `rbd` 程序导致 RBD 卷无法正常创建。

目前解决这个问题的办法是:
1. 在`kube-controller-manager` 中安装`ceph-common`
2. 安装第三方的 RBD Provisioner （https://github.com/kubernetes-incubator/external-storage/tree/master/ceph/rbd/deploy ），值得注意的是需要配置好有关 Pod 的命名空间和对应的 ServiceAccount Name，不然很可能因为 Provisioner 没有足够的 Kubernetes 权限导致 RBD 卷无法正常创建

### Ubuntu 16.04 下 RBD 卷无法挂载的问题
在创建 PVC 过程中，RBD Provisioner 可能会返回如下错误：

> [26655.270076] libceph: mon0 192.168.1.131:6789 missing required protocol features
> [26665.183987] libceph: mon1 192.168.1.132:6789 feature set mismatch, my 102b84a842a42 < server's 40102b84a842a42, missing 400000000000000

这个问题似乎是由于内核版本不够高导致一些 Ceph 需要的特性没有得到支持（ http://cephnotes.ksperis.com/blog/2014/01/21/feature-set-mismatch-error-on-ceph-kernel-client/ ， http://bbs.ceph.org.cn/question/680 ）。

修复办法是修改 `CRUSH MAP` 的配置， 将 `chooseleaf_vary_r` 与 `chooseleaf_stable` 设为 `0`：

```bash
ceph osd getcrushmap -o crush
crushtool -i crush --set-chooseleaf_vary_r 0  --set-chooseleaf_stable 0 -o crush.new
ceph osd setcrushmap -i crush.new
rbd map -p pool1 bd1
```

引自此文： https://github.com/grzhan/keng/issues/2
