# ceph
[pg 计算器](https://ceph.com/pgcalc/)

## ARCHITECTURE

### rados 系统逻辑
在使用RADOS系统时，客户端程序通过与OSD或者Monitor的交互获取ClusterMap，然后直接在本地进行计算，得出对象的存储位置后，便直接与对应的OSD通信，完成数据的各种操作。可见，在此过程中，只要保证ClusterMap不频繁更新，则客户端显然可以不依赖于任何元数据服务器，不进行任何查表操作，便完成数据访问流程。

地址寻址： 三段映射  

![基于计算的对象寻址机制](https://img.kancloud.cn/4a/2b/4a2b056e6e3664c9909b469cdc47c97e_3646x1807.png)
- file -- > object  
这次映射的目的是，将用户要操作的File，映射为RADOS能够处理的Object,并且分配ID。其映射十分简单，本质上就是按照object的最大size对file进行切分，相当于RAID中的条带化过程。这种切分的好处有二。  
一是让大小不限的File变成最大Size一致、可以被RADOS高效管理的Object。  
二是让对单一File实施的串行处理变为对多个Object实施的并行化处理。

- object -- > PG  
在File被映射为一个或多个Object之后，就需要将每个Object独立地映射到一个PG中去。这个映射过程也很简单，其计算公式是(Hash(OID) & Mask -> PGID)。

- PG -- > OSD映射
第三次映射就是将作为Object的逻辑组织单元的PG映射到数据的实际存储单元OSD。如图所示，RADOS采用一个名为CRUSH的算法，将PGID代入其中，然后得到一组共N个OSD。这N个OSD即共同负责存储和维护一个PG中的所有Object。 NOTES： 这次映射功能特别强大，直接受到cluster map和rule的影响，只有在cluster map和存储策略均不发生改变时，pg和osd映射关系才是固定的。

![](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/oRL2fUHmGZA0rG7tMj2EA35vuVGHeXfLBS8acAlRcjuR1Xj6pe2Rn1AW53icziaRgEIZd1Ppe7P9QDhHAnRx7Ptg/640?wx_fmt=png)



### ceph
![stack](https://docs.ceph.com/en/latest/_images/stack.png)
### STORING DATA

![](https://docs.ceph.com/en/latest/_images/58ad76633b811722da14b103fd40949c907d83320d3b1717a6ccc127dae4a529.png)

### Replication
![](https://docs.ceph.com/en/latest/_images/6324fc870f294e5e407722de1ff965eb6b8a40a55c1b512174d7f3ecbd7b4b56.png)

### about pools
![](https://docs.ceph.com/en/latest/_images/6bd81b732befb17e664371189141fdf2c18e032d2c0f87b2a29f8fb78f895f78.png)

Ceph存储系统支持“池”的概念，池是用于存储对象的逻辑分区
ceph client  从 mon 获取 cluster map，并且写入object到pool中，pool 属性的size是复制分片的数量，CRUSH和 pg_num 决定了数据怎么存放。
Pools set at least the following parameters:
- Ownership/Access to Objects
- The Number of Placement Groups, and
- The CRUSH Rule to Use.

1. 所有权和访问对象
1. 对象副本的数目
1. pg的数目
1. crush rule set的使用

### mapping PGs to OSDS

![](https://docs.ceph.com/en/latest/_images/f592d64bd19e67476c118c14caf9d4e3df61607d25670d5b3e83b45d2f29db99.png)
每个pool都有一堆的pgs，CRUSH 动态map PGs 到osd上，当client存储object时，CRUSH将决定object map到那个pg上。
(pool,PG) --> osd set 的映射由四个因素决定：
- CRUSH算法
- OSD map：包含当前所有pool的状态和所有osd的状态
- CRUSH map： 包含当前磁盘，机器，机架的层级结构
- CRUSH rule：数据映射策略




### 计算PG IDS
client从mon中获取 cluster map，从而了解mon，osd和metadata server。但是client并不知道关于object的位置。  
``
Object locations get computed.
``  
The only input required by the client is the object ID and the pool. It’s simple: Ceph stores data in named pools (e.g., “liverpool”). When a client wants to store a named object (e.g., “john,” “paul,” “george,” “ringo”, etc.) it calculates a placement group using the object name, a hash code, the number of PGs in the pool and the pool name. Ceph clients use the following steps to compute PG IDs.

The client inputs the pool name and the object ID. (e.g., pool = “liverpool” and object-id = “john”)

- Ceph takes the object ID and hashes it.

- Ceph calculates the hash modulo the number of PGs. (e.g., 58) to get a PG ID.

- Ceph gets the pool ID given the pool name (e.g., “liverpool” = 4)

- Ceph prepends the pool ID to the PG ID (e.g., 4.58).

Computing object locations is much faster than performing object location query over a chatty session. The CRUSH algorithm allows a client to compute where objects should be stored, and enables the client to contact the primary OSD to store or retrieve the objects.

## 安装
略

## 命令

### rbd

- 创建pool    
ceph osd  pool create {poolname} pg_size pgs_size
- 列出所有的pool    
rados lspools 或者 ceph osd lspools

- 创建image    
rbd create imagename --size {megabytes} --pool  poolname

- 列出pool中的image    
rbd ls  {poolname}

- 检索块设备    
rbd --image {poolname/imagename} info

- 更改块设备的大小（只能增大），k8s里可以通过调整 `capacity`的值来调整大小    
rbd resize  --image {poolname/imagename} --size {M}

- 文件系统在线扩容  
resize2fs /dev/rbd0

- 删除块设备    
rbd rm {image-name}

- 删除pool    
ceph osd  pool rm poolname poolname --yes-i-really-really-mean-it

- 映射块设备  
 rbd map {imagename} --pool poolname --id {user-name}

- 查看已映射的块设备  
rbd shownmapped

- 取消映射  
rbd unmap /dev/rbd/{poolname}/{imagename}

### rados

- 集群使用率  
rados df

- 打印pg  
ceph pg dump

- 列出指定pg的object  
ceph rados --pgid 0.6 ls

- 写入一个object  
rados -p foo put myobject blah.txt

- 删除object  
rados -p foo rm myobject

- 列出pool中的object 对象  
rados -p {poolname} ls -

### 快照和克隆
- 创建快照
rbd --pool poolname snap create --snap {snapname} {imagename} 或者  
rbd snap create {poolname}/{imagename}@{snapname}

- 快照回滚  
rbd --pool {poolname} snap rollback --snap {snapname} {imagename} 或者
rbd snap rollback {poolname}/{imagename}@{snapname}

- 快照清除  
rbd --pool {poolname} snap purge {imagename} 或者  
rbd snap purge {poolname}/imagename


- 删除快照  
rbd --pool {poolname} snap rm {imagename}  或者  
rbd snap rm {poolname}/{imagename}@{snapname}  

- 克隆  
rbd clone {poolname}/{imagename}@{snapname} -snap {poolname}/{imagename} --snap-clone  

- 查看快照的克隆  
rbd --pool {poolname} children --image {imagename} --snap {snapname}  或者   
rbd children {poolname}/{imagename}@{snapname}



### 认证
- 列出用户和权限  
ceph auth ls

- 获取指定用户信息  
ceph auth get client.admin

- 新建用户  

  ```bash
  # 授权的格式为：`{守护进程类型} 'allow {权限}'`

  # 创建 client.john 用户，并授权可读 MON、可读写 OSD Pool livepool
  ceph auth add client.john mon 'allow r' osd 'allow rw pool=liverpool'

  # 获取或创建 client.paul，若创建，则授权可读 MON，可读写 OSD Pool livepool
  ceph auth get-or-create client.paul mon 'allow r' osd 'allow rw pool=liverpool'

  # 获取或创建 client.george，若创建，则授权可读 MON，可读写  OSD Pool livepool，输出用户秘钥环文件
  ceph auth get-or-create client.george mon 'allow r' osd 'allow rw pool=liverpool' -o george.keyring
  ceph auth get-or-create-key client.ringo mon 'allow r' osd 'allow rw pool=liverpool' -o ringo.key

  ```

- 为指定用户授权

  ```bash
  # 格式：ceph auth caps USERTYPE.USERID {daemon} 'allow [r|w|x|*|...] [pool={pool-name}] [namespace={namespace-name}]' [{daemon} 'allow [r|w|x|*|...] [pool={pool-name}] [namespace={namespace-name}]']

  ceph auth get client.john
  ceph auth caps client.john mon 'allow r' osd 'allow rw pool=liverpool'
  ceph auth caps client.paul mon 'allow rw' osd 'allow rwx pool=liverpool'
  ceph auth caps client.brian-manager mon 'allow *' osd 'allow *'
  ```

- 删除用户

  ```bash
  ceph auth del {TYPE}.{ID}
  ```


### crush
`controlled,scalable,Decentralized placement of Replicated data`  
[管理crushmap](https://lihaijing.gitbooks.io/ceph-handbook/content/Operation/manage_crushmap.html)    
[redhat CRUSH ADMINISTRATION bucket算法](https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/2/html/storage_strategies_guide/crush_administration)  
[ceph crush官方文档](https://docs.ceph.com/en/latest/rados/operations/crush-map-edits/)

cursh的层级结构：
![](https://access.redhat.com/webassets/avalon/d/Red_Hat_Ceph_Storage-2-Storage_Strategies_Guide-en-US/images/715c42e85c8658debb446348d6573af7/diag-a5344305ec87e1d1de603d3ab255210a.png)
CRUSH Map 主要有 4 个段落：
1. 设备：由任意对象存储设备组成，即对应一个 ceph-osd进程的存储器。 Ceph 配置文件里的每个 OSD 都应该有一个设备。  
  为把 PG 映射到 OSD ， CRUSH Map 需要 OSD 列表（即配置文件所定义的 OSD 守护进程名称），所以它们首先出现在 CRUSH Map 里。要在 CRUSH Map 里声明一个设备，在设备列表后面新建一行，输入 device 、之后是唯一的数字 ID 、之后是相应的 ceph-osd 守护进程实例名字。
1. 桶类型： 定义了 CRUSH 分级结构里要用的桶类型（ types ），桶由逐级汇聚的存储位置（如行、机柜、机箱、主机等等）及其权重组成。  
CRUSH Map 里的第二个列表定义了 bucket （桶）类型，桶简化了节点和叶子层次。节点（或非叶子）桶在分级结构里一般表示物理位置，节点汇聚了其它节点或叶子，叶桶表示 ceph-osd 守护进程及其对应的存储媒体。
要往 CRUSH Map 中增加一种 bucket 类型，在现有桶类型列表下方新增一行，输入 type 、之后是惟一数字 ID 和一个桶名。按惯例，会有一个叶子桶为 type 0 ，然而你可以指定任何名字（如 osd 、 disk 、 drive 、 storage 等等）：
1. 桶实例： 定义了桶类型后，还必须声明主机的桶类型、以及规划的其它故障域。
1. 规则： cursh如何选择bucket的规则。由选择桶的方法组成。


- crush 一致性hash
  - 故障域隔离。数据不同副本分布在不同的故障域，降低数据损坏的风险。
  - 负载均衡。数据能够均匀分布在磁盘容量不等的存储节点，避免部分节点空闲，或者超载。
  - 控制节点加入或者离开时引起的数据迁移量。只有离线的节点上的数据才会发生迁移。

- 获取crush map  
ceph osd getcrushmap -o crush #获取crush的二进制文件，需要反编译
- 反编译  
crushtool -d crush -o crush.txt
- 编译 CRUSH Map  
crushtool -c {decompiled-crush-map-filename} -o {compiled-crush-map-filename}

- 注入一个crush map  
ceph osd setcrushmap -i  {compiled-crushmap-filename}


### 查找osd在哪个bucket
 ceph osd find osd.1
### 更换硬盘
1. kill osd daemon  
ceph osd down osdid
1. mark osd out  
ceph osd out osdid
1. remove from CRUSH map  
ceph osd crush  rm osd.x
1. delete authentication keys  
ceph auth del osd.x
1. remove osd from cluster  
ceph osd rm osd.x
## 存储提供
略

## 扩展阅读

[一致性hash](https://zhuanlan.zhihu.com/p/60963885)
